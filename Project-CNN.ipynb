{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Dataset and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images: ✅\n",
      "image paths: ✅\n",
      "Automobiles: ✅\n",
      "image pixels: ✅\n",
      "Shape of image: ✅\n"
     ]
    }
   ],
   "source": [
    "#loading files \n",
    "\n",
    "import os\n",
    "import re\n",
    "import cv2\n",
    "from keras.preprocessing.image import img_to_array \n",
    "\n",
    "#images \n",
    "images = []\n",
    "#automobile names \n",
    "automobile = [] \n",
    "#path to image \n",
    "images_path = []\n",
    "#pixel values of images \n",
    "images_pixels = []\n",
    "#label numerical value associated to image \n",
    "labels = []\n",
    "dict = {}\n",
    "\n",
    "i=0 \n",
    "path = [\"./Bus\", \"./Car\", \"./Bicycle\", \"./Motorcycle\"] \n",
    "# Bus - 1229 images \n",
    "# Car - 3578 images \n",
    "# Bicycle - 800 images\n",
    "# Motorcycle - 101 images\n",
    "# Total - 5708\n",
    "\n",
    "for dir_path in path: \n",
    "    #check if dir_path exists \n",
    "    if os.path.isdir(dir_path): \n",
    "        automobile.append(dir_path[2:])\n",
    "        for img in os.listdir(dir_path): \n",
    "            #all images must be png \n",
    "            if len(re.findall('.png',img.lower())) != 0: \n",
    "                img_path = os.path.join(dir_path,img) \n",
    "                images.append(img) \n",
    "                images_path.append(img_path) \n",
    "                img_pix = cv2.imread(img_path,1) \n",
    "                #all images must be 120x120 \n",
    "                images_pixels.append(cv2.resize(img_pix, (120, 120)))\n",
    "                labels.append(i) \n",
    "    i = i+1 \n",
    "\n",
    "#check \n",
    "print(\"images: ✅\" if len(images) == 5708 else f\"images: ❌\\nError: {len(images)} =/= 5708\") \n",
    "print(\"image paths: ✅\" if len(images_path) == 5708 else f\"image paths: ❌\\nError: {len(images_path)} =/= 5708\") \n",
    "print(\"Automobiles: ✅\" if len(automobile) == 4 else f\"images: ❌\\nError: {len(automobile)} =/= 4\") \n",
    "print(\"image pixels: ✅\" if len(images_pixels) == 5708 else f\"image pixels: ❌\\nError: {len(images_pixels)} =/= 5708\") \n",
    "print(\"Shape of image: ✅\" if images_pixels[0].shape == (120, 120, 3) else f\"Shape of image: ❌\\nError: {images_pixels[0].shape} =/= (120, 120, 3)\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "shuffled = list(zip(images_pixels, labels))\n",
    "random.shuffle(shuffled) \n",
    "\n",
    "train_data, labels_data = zip(*shuffled) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "import numpy as np \n",
    "\n",
    "X_data = np.array(train_data)/255 \n",
    "Y_data = to_categorical(labels_data, num_classes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_data, Y_data, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape:  (4566, 120, 120, 3)\n",
      "Y_train shape:  (4566, 4)\n",
      "X_test shape:  (1142, 120, 120, 3)\n",
      "Y_test shape:  (1142, 4)\n"
     ]
    }
   ],
   "source": [
    "#cehck shape of X_train, X_test, Y_train, Y_test \n",
    "print(\"X_train shape: \", X_train.shape) \n",
    "print(\"Y_train shape: \", Y_train.shape) \n",
    "print(\"X_test shape: \", X_test.shape) \n",
    "print(\"Y_test shape: \", Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset \n",
    "\n",
    "class CustomDataset(Dataset): \n",
    "    def __init__(self, X, Y, transform=None): \n",
    "        self.X = X \n",
    "        self.Y = Y \n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.X) \n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        x_sample = self.X[idx]\n",
    "        y_sample = self.Y[idx]\n",
    "\n",
    "        if self.transform: \n",
    "            x_sample = self.transform(x_sample) \n",
    "        \n",
    "        return x_sample, y_sample \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([64, 3, 120, 120])\n",
      "Labels batch shape: torch.Size([64, 4])\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms \n",
    "\n",
    "img_transform = transforms.Compose([\n",
    "    transforms.ToTensor(), \n",
    "])\n",
    "\n",
    "train_dataset = CustomDataset(X_train, Y_train, transform=img_transform)\n",
    "test_dataset = CustomDataset(X_test, Y_test, transform=img_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True) \n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "image_check, labels_check = next(iter(train_loader))\n",
    "print(f\"Feature batch shape: {image_check.size()}\")\n",
    "print(f\"Labels batch shape: {labels_check.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of CrossEntropyLoss batch  1 : 73.44%\n",
      "Accuracy of CrossEntropyLoss batch  2 : 65.62%\n",
      "Accuracy of CrossEntropyLoss batch  3 : 66.02%\n",
      "Accuracy of CrossEntropyLoss batch  4 : 75.39%\n",
      "Accuracy of CrossEntropyLoss batch  5 : 72.27%\n",
      "Accuracy of CrossEntropyLoss batch  6 : 75.00%\n",
      "Accuracy of CrossEntropyLoss batch  7 : 74.22%\n",
      "Accuracy of CrossEntropyLoss batch  8 : 75.00%\n",
      "Accuracy of CrossEntropyLoss batch  9 : 74.22%\n",
      "Accuracy of CrossEntropyLoss batch  10 : 68.36%\n",
      "Accuracy of CrossEntropyLoss batch  11 : 64.84%\n",
      "Accuracy of CrossEntropyLoss batch  12 : 74.61%\n",
      "Accuracy of CrossEntropyLoss batch  13 : 73.44%\n",
      "Accuracy of CrossEntropyLoss batch  14 : 52.34%\n",
      "Accuracy of CrossEntropyLoss batch  15 : 73.83%\n",
      "Accuracy of CrossEntropyLoss batch  16 : 75.00%\n",
      "Accuracy of CrossEntropyLoss batch  17 : 60.16%\n",
      "Accuracy of CrossEntropyLoss batch  18 : 64.81%\n"
     ]
    }
   ],
   "source": [
    "from torch import nn, optim \n",
    "\n",
    "class NeuralNetwork(nn.Module): \n",
    "    def __init__(self): \n",
    "        super().__init__() \n",
    "        #creating sequential to utilize for optimizer parameters\n",
    "        self.cnn_stack = nn.Sequential( \n",
    "            # applies a 2D convolution over an input signal composed of several input planes, inchannels=3 since we're using RGB, outchannels=16 since we're using 16 filters \n",
    "            # kernel_size=3 since we're using 3x3 filter, padding=1 since we're using 3x3 filter\n",
    "            # input ex: [64, 3, 100, 100]\n",
    "            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),\n",
    "            # applies Batch Normalization over a 4D input (a mini-batch of 3D inputs with additional channel dimension), \n",
    "            # since we're directly going from convolutional layer the outchannel = inchannel \n",
    "            nn.BatchNorm2d(16), \n",
    "            # to introduce non-linearity helping neural networks learn a wide variety of phenomena (converts negative values to 0)\n",
    "            nn.ReLU(inplace=True), \n",
    "            # applies a 2D max pooling over an input signal composed of several input planes to reduce the size of image (since it's 2 we went from 100 -> 100/2)\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # applies a ２D convolution over an input signal composed of several input planes, since there's no change in channels the input channel will be 16 and the output will be the same\n",
    "            nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1),\n",
    "            # again, converting negative values into 0 \n",
    "            nn.ReLU(inplace=True), \n",
    "            # applies a 2D max pooling over an input signal composed of several input planes (50 -> 50/2) \n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # applies linear transformation on the input using its stored weights and biases <-- basically linear transformation for weight*height, label classes \n",
    "            # since we utilized 2 MaxPool2d we'll be getting (100/2)/2 and end up with 25*25 along with the channels it'll be 625*16 = 10000 with 9 classes it can map to \n",
    "            # input ex: [64, 16, 25, 25] \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(10000, 9) \n",
    "        )\n",
    "    \n",
    "    def forward(self, x) : \n",
    "        #x: [64, 3, 120, 120] \n",
    "        #print(\"x shape: \", x.shape) <-- [64, 3, 120, 120]\n",
    "        logits = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)(x) \n",
    "        #print(\"logits shape: \", logits.shape) <-- [64, 16, 120, 120]\n",
    "        logits = nn.BatchNorm2d(16)(logits) \n",
    "        #print(\"logits shape: \", logits.shape) <-- [64, 16, 120, 120]\n",
    "        logits = nn.ReLU(inplace=True)(logits)\n",
    "        #print(\"logits shape: \", logits.shape) <-- [64, 16, 120, 120]\n",
    "        logits = nn.MaxPool2d(kernel_size=2, stride=2)(logits)\n",
    "        #print(\"logits shape: \", logits.shape) <-- [64, 16, 60, 60]\n",
    "        logits = nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1)(logits)\n",
    "        #print(\"logits shape: \", logits.shape) <-- [64, 16, 60, 60]\n",
    "        logits = nn.ReLU(inplace=True)(logits)\n",
    "        #print(\"logits shape: \", logits.shape) <-- [64, 16, 60, 60]\n",
    "        logits = nn.MaxPool2d(kernel_size=2, stride=2)(logits)\n",
    "        #print(\"logits shape: \", logits.shape) <-- [64, 16, 30, 30]\n",
    "        #testing if flatten helps -> it works \n",
    "        logits = nn.Flatten()(logits)\n",
    "        #print(\"logits shape: \", logits.shape) <-- [64, 14400]\n",
    "        logits = nn.Linear(14400, 4)(logits)\n",
    "        return logits \n",
    "    \n",
    "model = NeuralNetwork() #testing CrossEntropyLoss\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.NAdam(model.parameters(), lr=0.001) \n",
    "\n",
    "model.train() \n",
    "for epoch in range(200): \n",
    "    for x_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad() \n",
    "        x_batch = x_batch.float() \n",
    "        y_pred = model(x_batch)\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        loss.backward() \n",
    "        optimizer.step()\n",
    "\n",
    "#test model \n",
    "model.eval()\n",
    "i = 1\n",
    "for x_test, y_test in test_loader: \n",
    "    x_test = x_test.float()\n",
    "    y_pred = model(x_test) \n",
    "    acc = (y_pred.round() == y_test).float().mean()\n",
    "    acc = float(acc) \n",
    "    print(\"Accuracy of CrossEntropyLoss batch \", i, \": %.2f%%\" % (acc*100)) \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of HingeEntropyLoss batch  1 : 74.61%\n",
      "Accuracy of HingeEntropyLoss batch  2 : 52.73%\n",
      "Accuracy of HingeEntropyLoss batch  3 : 76.17%\n",
      "Accuracy of HingeEntropyLoss batch  4 : 64.06%\n",
      "Accuracy of HingeEntropyLoss batch  5 : 74.61%\n",
      "Accuracy of HingeEntropyLoss batch  6 : 74.61%\n",
      "Accuracy of HingeEntropyLoss batch  7 : 73.83%\n",
      "Accuracy of HingeEntropyLoss batch  8 : 72.27%\n",
      "Accuracy of HingeEntropyLoss batch  9 : 74.61%\n",
      "Accuracy of HingeEntropyLoss batch  10 : 74.22%\n",
      "Accuracy of HingeEntropyLoss batch  11 : 75.00%\n",
      "Accuracy of HingeEntropyLoss batch  12 : 75.00%\n",
      "Accuracy of HingeEntropyLoss batch  13 : 74.61%\n",
      "Accuracy of HingeEntropyLoss batch  14 : 73.44%\n",
      "Accuracy of HingeEntropyLoss batch  15 : 73.83%\n",
      "Accuracy of HingeEntropyLoss batch  16 : 73.83%\n",
      "Accuracy of HingeEntropyLoss batch  17 : 74.22%\n",
      "Accuracy of HingeEntropyLoss batch  18 : 56.94%\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork() #testing HingeEmbeddingLoss\n",
    "\n",
    "loss_fn = nn.HingeEmbeddingLoss()\n",
    "\n",
    "optimizer = optim.NAdam(model.parameters(), lr=0.001) \n",
    "\n",
    "model.train() \n",
    "for epoch in range(200): \n",
    "    for x_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad() \n",
    "        x_batch = x_batch.float() \n",
    "        y_pred = model(x_batch)\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        loss.backward() \n",
    "        optimizer.step() \n",
    "\n",
    "#test model \n",
    "model.eval()\n",
    "i = 1\n",
    "for x_test, y_test in test_loader: \n",
    "    x_test = x_test.float()\n",
    "    y_pred = model(x_test) \n",
    "    acc = (y_pred.round() == y_test).float().mean()\n",
    "    acc = float(acc) \n",
    "    print(\"Accuracy of HingeEntropyLoss batch \", i, \": %.2f%%\" % (acc*100)) \n",
    "    i += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
